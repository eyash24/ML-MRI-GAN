{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator 2\n",
    "\n",
    "class Unet(nn.Module):\n",
    "\n",
    "    def __init__(self,in_dim=1,conv_dim=64,out_dim=1):\n",
    "        super(Unet, self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv2d(in_dim,conv_dim,kernel_size=3,stride=2,padding=1), #64\n",
    "            nn.BatchNorm2d(conv_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(conv_dim,conv_dim*2,kernel_size=3,stride=2,padding=1), #32\n",
    "            nn.BatchNorm2d(conv_dim*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 =nn.Sequential(\n",
    "            nn.Conv2d(conv_dim*2, conv_dim * 4, kernel_size=3, stride=2, padding=1), #16\n",
    "            nn.BatchNorm2d(conv_dim * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(conv_dim * 4, conv_dim * 8, kernel_size=3, stride=2, padding=1), #8\n",
    "            nn.BatchNorm2d(conv_dim * 8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.deconv1=nn.Sequential(\n",
    "            nn.ConvTranspose2d(conv_dim * 8,conv_dim * 8,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.BatchNorm2d(conv_dim * 8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.deconv2=nn.Sequential(\n",
    "            nn.ConvTranspose2d(conv_dim * (8+4),conv_dim * 4,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.BatchNorm2d(conv_dim * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.deconv3=nn.Sequential(\n",
    "            nn.ConvTranspose2d(conv_dim * (4+2),conv_dim * 2,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.BatchNorm2d(conv_dim * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.deconv4=nn.Sequential(\n",
    "            nn.ConvTranspose2d(conv_dim * (2+1),out_dim ,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0)\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "            if isinstance(m,nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1=self.conv1(x)\n",
    "        x2=self.conv2(x1)\n",
    "        x3=self.conv3(x2)\n",
    "        x4=self.conv4(x3)\n",
    "        out=self.deconv1(x4)\n",
    "        x3=torch.cat([x3,out],dim=1)\n",
    "        out=self.deconv2(x3)\n",
    "        x2 = torch.cat([x2, out], dim=1)\n",
    "        out=self.deconv3(x2)\n",
    "        x1=torch.cat([x1,out],dim=1)\n",
    "        out=self.deconv4(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((1, 1, 128, 128))\n",
    "unet=Unet()\n",
    "y=unet(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Unet                                     [1, 1, 128, 128]          --\n",
       "├─Sequential: 1-1                        [1, 64, 64, 64]           --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 64, 64]           640\n",
       "│    └─BatchNorm2d: 2-2                  [1, 64, 64, 64]           128\n",
       "│    └─ReLU: 2-3                         [1, 64, 64, 64]           --\n",
       "├─Sequential: 1-2                        [1, 128, 32, 32]          --\n",
       "│    └─Conv2d: 2-4                       [1, 128, 32, 32]          73,856\n",
       "│    └─BatchNorm2d: 2-5                  [1, 128, 32, 32]          256\n",
       "│    └─ReLU: 2-6                         [1, 128, 32, 32]          --\n",
       "├─Sequential: 1-3                        [1, 256, 16, 16]          --\n",
       "│    └─Conv2d: 2-7                       [1, 256, 16, 16]          295,168\n",
       "│    └─BatchNorm2d: 2-8                  [1, 256, 16, 16]          512\n",
       "│    └─ReLU: 2-9                         [1, 256, 16, 16]          --\n",
       "├─Sequential: 1-4                        [1, 512, 8, 8]            --\n",
       "│    └─Conv2d: 2-10                      [1, 512, 8, 8]            1,180,160\n",
       "│    └─BatchNorm2d: 2-11                 [1, 512, 8, 8]            1,024\n",
       "│    └─ReLU: 2-12                        [1, 512, 8, 8]            --\n",
       "├─Sequential: 1-5                        [1, 512, 16, 16]          --\n",
       "│    └─ConvTranspose2d: 2-13             [1, 512, 16, 16]          2,359,808\n",
       "│    └─BatchNorm2d: 2-14                 [1, 512, 16, 16]          1,024\n",
       "│    └─ReLU: 2-15                        [1, 512, 16, 16]          --\n",
       "├─Sequential: 1-6                        [1, 256, 32, 32]          --\n",
       "│    └─ConvTranspose2d: 2-16             [1, 256, 32, 32]          1,769,728\n",
       "│    └─BatchNorm2d: 2-17                 [1, 256, 32, 32]          512\n",
       "│    └─ReLU: 2-18                        [1, 256, 32, 32]          --\n",
       "├─Sequential: 1-7                        [1, 128, 64, 64]          --\n",
       "│    └─ConvTranspose2d: 2-19             [1, 128, 64, 64]          442,496\n",
       "│    └─BatchNorm2d: 2-20                 [1, 128, 64, 64]          256\n",
       "│    └─ReLU: 2-21                        [1, 128, 64, 64]          --\n",
       "├─Sequential: 1-8                        [1, 1, 128, 128]          --\n",
       "│    └─ConvTranspose2d: 2-22             [1, 1, 128, 128]          1,729\n",
       "│    └─Sigmoid: 2-23                     [1, 1, 128, 128]          --\n",
       "==========================================================================================\n",
       "Total params: 6,127,297\n",
       "Trainable params: 6,127,297\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.49\n",
       "==========================================================================================\n",
       "Input size (MB): 0.07\n",
       "Forward/backward pass size (MB): 22.68\n",
       "Params size (MB): 24.51\n",
       "Estimated Total Size (MB): 47.25\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=unet,input_size=(1,1,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "Layer 1: \ttorch.Size([1, 1, 256, 256])\ttorch.Size([1, 16, 128, 128])\n",
      "Layer 2: \ttorch.Size([1, 16, 128, 128])\ttorch.Size([1, 16, 64, 64])\n",
      "Layer 3: \ttorch.Size([1, 16, 64, 64])\ttorch.Size([1, 32, 32, 32])\n",
      "Layer 4: \ttorch.Size([1, 32, 32, 32])\ttorch.Size([1, 64, 16, 16])\n",
      "Layer 5: \ttorch.Size([1, 64, 16, 16])\ttorch.Size([1, 128, 8, 8])\n",
      "Layer 6: \ttorch.Size([1, 128, 8, 8])\ttorch.Size([1, 256, 4, 4])\n",
      "Layer 7: \ttorch.Size([1, 256, 4, 4])\ttorch.Size([1, 512, 4, 4])\n",
      "Layer 8: \ttorch.Size([1, 512, 4, 4])\ttorch.Size([1, 512, 4, 4])\n",
      "\n",
      "Layer 9: \ttorch.Size([1, 512, 4, 4])\ttorch.Size([1, 512, 4, 4])\n",
      "Layer 10: \ttorch.Size([1, 512, 4, 4])\ttorch.Size([1, 256, 4, 4])\n",
      "Layer 11: \ttorch.Size([1, 256, 4, 4])\ttorch.Size([1, 128, 8, 8])\n",
      "Layer 12: \ttorch.Size([1, 128, 8, 8])\ttorch.Size([1, 64, 16, 16])\n",
      "Layer 13: \ttorch.Size([1, 64, 16, 16])\ttorch.Size([1, 32, 32, 32])\n",
      "Layer 14: \ttorch.Size([1, 32, 32, 32])\ttorch.Size([1, 16, 64, 64])\n",
      "Layer 15: \ttorch.Size([1, 16, 64, 64])\ttorch.Size([1, 16, 128, 128])\n",
      "Layer 16: \ttorch.Size([1, 16, 128, 128])\ttorch.Size([1, 1, 256, 256])\n",
      "Layer 17: \ttorch.Size([1, 1, 256, 256])\ttorch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# G1 testing \n",
    "\n",
    "xb = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "print(xb.shape)\n",
    "# Downsampling \n",
    "c0 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "c1 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "c2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "c3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "c4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "c5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "c6 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "c7 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "# Upsampling\n",
    "c8 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "c9 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "# c10 = nn.ConvTranspose2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "c10 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "# c12 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "c11 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "c12 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "c13 = nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "# c16 = nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "c14 = nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "c15 = nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "c16= nn.Tanh()\n",
    "\n",
    "\n",
    "out = c0(xb)\n",
    "print(\"Layer 1: \", xb.shape, out.shape, sep=\"\\t\")\n",
    "\n",
    "c = [c1,c2, c3, c4, c5, c6, c7,c8,c9,c10,c11,c12,c13,c14,c15,c16]\n",
    "\n",
    "for n, layer in enumerate(c):\n",
    "    out_in = out\n",
    "    out = layer(out)\n",
    "    print(f\"Layer {n+2}: \",out_in.shape, out.shape, sep=\"\\t\")\n",
    "    if layer in [c7]:\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:  torch.Size([1, 16, 128, 128])\n",
      "x1:  torch.Size([1, 16, 64, 64])\n",
      "x2:  torch.Size([1, 32, 32, 32])\n",
      "x3:  torch.Size([1, 64, 16, 16])\n",
      "x4:  torch.Size([1, 128, 8, 8])\n",
      "x5:  torch.Size([1, 256, 4, 4])\n",
      "x6:  torch.Size([1, 512, 4, 4])\n",
      "x7:  torch.Size([1, 512, 4, 4])\n",
      "x8:  torch.Size([1, 512, 4, 4])\n",
      "x9:  torch.Size([1, 256, 4, 4])\n",
      "x10:  torch.Size([1, 128, 8, 8])\n",
      "x11:  torch.Size([1, 64, 16, 16])\n",
      "x12:  torch.Size([1, 32, 32, 32])\n",
      "x13:  torch.Size([1, 16, 64, 64])\n",
      "x14:  torch.Size([1, 16, 128, 128])\n",
      "x15:  torch.Size([1, 1, 256, 256])\n",
      "x16:  torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "xb = torch.randn(1, 1, 256, 256)\n",
    "\n",
    "x0 = c0(xb)\n",
    "print(\"x0: \", x0.shape)\n",
    "x1 = c1(x0)\n",
    "print(\"x1: \", x1.shape)\n",
    "x2 = c2(x1)\n",
    "print(\"x2: \", x2.shape)\n",
    "x3 = c3(x2)\n",
    "print(\"x3: \", x3.shape)\n",
    "x4 = c4(x3)\n",
    "print(\"x4: \", x4.shape)\n",
    "x5 = c5(x4)\n",
    "print(\"x5: \", x5.shape)\n",
    "x6 = c6(x5)\n",
    "print(\"x6: \", x6.shape)\n",
    "x7 = c7(x6)\n",
    "print(\"x7: \", x7.shape)\n",
    "x8 = c8(x7)\n",
    "print(\"x8: \", x8.shape)\n",
    "x9 = c9(x8) + x5\n",
    "print(\"x9: \", x9.shape)\n",
    "x10 = c10(x9) + x4\n",
    "print(\"x10: \", x10.shape)\n",
    "x11 = c11(x10) + x3\n",
    "print(\"x11: \", x11.shape)\n",
    "x12 = c12(x11) + x2\n",
    "print(\"x12: \", x12.shape)\n",
    "x13 = c13(x12) + x1\n",
    "print(\"x13: \", x13.shape)\n",
    "x14 = c14(x13) \n",
    "print(\"x14: \", x14.shape)\n",
    "x15 = c15(x14) \n",
    "print(\"x15: \", x15.shape)\n",
    "x16 = c16(x15) \n",
    "print(\"x16: \", x16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGen(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(UnetGen, self).__init__()\n",
    "        \n",
    "        self.c0 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        # Upsample\n",
    "        \n",
    "        self.c8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c9 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c10 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c11 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c12 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c13 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c14 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c15 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(inplace=True)         \n",
    "        )\n",
    "        \n",
    "        self.c16 = nn.Tanh()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = self.c0(x)\n",
    "        x1 = self.c1(x0)\n",
    "        x2 = self.c2(x1)\n",
    "        x3 = self.c3(x2)\n",
    "        x4 = self.c4(x3)\n",
    "        x5 = self.c5(x4)\n",
    "        x6 = self.c6(x5)\n",
    "        x7 = self.c7(x6)\n",
    "        x8 = self.c8(x7)\n",
    "        x9 = self.c9(x8) + x5\n",
    "        x10 = self.c10(x9) + x4\n",
    "        x11 = self.c11(x10) + x3\n",
    "        x12 = self.c12(x11) + x2\n",
    "        x13 = self.c13(x12) + x1\n",
    "        x14 = self.c14(x13) \n",
    "        x15 = self.c15(x14) \n",
    "        x16 = self.c16(x15) \n",
    "\n",
    "        return x16\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((32, 1, 256, 256))\n",
    "unetgen=UnetGen()\n",
    "y=unetgen(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "UnetGen                                  [1, 1, 256, 256]          --\n",
       "├─Sequential: 1-1                        [1, 16, 128, 128]         --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 128, 128]         144\n",
       "│    └─BatchNorm2d: 2-2                  [1, 16, 128, 128]         32\n",
       "│    └─ReLU: 2-3                         [1, 16, 128, 128]         --\n",
       "├─Sequential: 1-2                        [1, 16, 64, 64]           --\n",
       "│    └─Conv2d: 2-4                       [1, 16, 64, 64]           2,304\n",
       "│    └─BatchNorm2d: 2-5                  [1, 16, 64, 64]           32\n",
       "│    └─ReLU: 2-6                         [1, 16, 64, 64]           --\n",
       "├─Sequential: 1-3                        [1, 32, 32, 32]           --\n",
       "│    └─Conv2d: 2-7                       [1, 32, 32, 32]           4,608\n",
       "│    └─BatchNorm2d: 2-8                  [1, 32, 32, 32]           64\n",
       "│    └─ReLU: 2-9                         [1, 32, 32, 32]           --\n",
       "├─Sequential: 1-4                        [1, 64, 16, 16]           --\n",
       "│    └─Conv2d: 2-10                      [1, 64, 16, 16]           18,432\n",
       "│    └─BatchNorm2d: 2-11                 [1, 64, 16, 16]           128\n",
       "│    └─ReLU: 2-12                        [1, 64, 16, 16]           --\n",
       "├─Sequential: 1-5                        [1, 128, 8, 8]            --\n",
       "│    └─Conv2d: 2-13                      [1, 128, 8, 8]            73,728\n",
       "│    └─BatchNorm2d: 2-14                 [1, 128, 8, 8]            256\n",
       "│    └─ReLU: 2-15                        [1, 128, 8, 8]            --\n",
       "├─Sequential: 1-6                        [1, 256, 4, 4]            --\n",
       "│    └─Conv2d: 2-16                      [1, 256, 4, 4]            294,912\n",
       "│    └─BatchNorm2d: 2-17                 [1, 256, 4, 4]            512\n",
       "│    └─ReLU: 2-18                        [1, 256, 4, 4]            --\n",
       "├─Sequential: 1-7                        [1, 512, 2, 2]            --\n",
       "│    └─Conv2d: 2-19                      [1, 512, 2, 2]            1,179,648\n",
       "│    └─BatchNorm2d: 2-20                 [1, 512, 2, 2]            1,024\n",
       "│    └─ReLU: 2-21                        [1, 512, 2, 2]            --\n",
       "├─Sequential: 1-8                        [1, 512, 1, 1]            --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 1, 1]            2,359,296\n",
       "│    └─BatchNorm2d: 2-23                 [1, 512, 1, 1]            1,024\n",
       "│    └─ReLU: 2-24                        [1, 512, 1, 1]            --\n",
       "├─Sequential: 1-9                        [1, 512, 1, 1]            --\n",
       "│    └─ConvTranspose2d: 2-25             [1, 512, 1, 1]            2,359,296\n",
       "│    └─BatchNorm2d: 2-26                 [1, 512, 1, 1]            1,024\n",
       "│    └─ReLU: 2-27                        [1, 512, 1, 1]            --\n",
       "├─Sequential: 1-10                       [1, 256, 1, 1]            --\n",
       "│    └─ConvTranspose2d: 2-28             [1, 256, 1, 1]            1,179,648\n",
       "│    └─BatchNorm2d: 2-29                 [1, 256, 1, 1]            512\n",
       "│    └─ReLU: 2-30                        [1, 256, 1, 1]            --\n",
       "├─Sequential: 1-11                       [1, 128, 8, 8]            --\n",
       "│    └─ConvTranspose2d: 2-31             [1, 128, 8, 8]            524,288\n",
       "│    └─BatchNorm2d: 2-32                 [1, 128, 8, 8]            256\n",
       "│    └─ReLU: 2-33                        [1, 128, 8, 8]            --\n",
       "├─Sequential: 1-12                       [1, 64, 16, 16]           --\n",
       "│    └─ConvTranspose2d: 2-34             [1, 64, 16, 16]           131,072\n",
       "│    └─BatchNorm2d: 2-35                 [1, 64, 16, 16]           128\n",
       "│    └─ReLU: 2-36                        [1, 64, 16, 16]           --\n",
       "├─Sequential: 1-13                       [1, 32, 32, 32]           --\n",
       "│    └─ConvTranspose2d: 2-37             [1, 32, 32, 32]           32,768\n",
       "│    └─BatchNorm2d: 2-38                 [1, 32, 32, 32]           64\n",
       "│    └─ReLU: 2-39                        [1, 32, 32, 32]           --\n",
       "├─Sequential: 1-14                       [1, 16, 64, 64]           --\n",
       "│    └─ConvTranspose2d: 2-40             [1, 16, 64, 64]           8,192\n",
       "│    └─BatchNorm2d: 2-41                 [1, 16, 64, 64]           32\n",
       "│    └─ReLU: 2-42                        [1, 16, 64, 64]           --\n",
       "├─Sequential: 1-15                       [1, 16, 128, 128]         --\n",
       "│    └─ConvTranspose2d: 2-43             [1, 16, 128, 128]         4,096\n",
       "│    └─BatchNorm2d: 2-44                 [1, 16, 128, 128]         32\n",
       "│    └─ReLU: 2-45                        [1, 16, 128, 128]         --\n",
       "├─Sequential: 1-16                       [1, 1, 256, 256]          --\n",
       "│    └─ConvTranspose2d: 2-46             [1, 1, 256, 256]          256\n",
       "│    └─BatchNorm2d: 2-47                 [1, 1, 256, 256]          2\n",
       "│    └─ReLU: 2-48                        [1, 1, 256, 256]          --\n",
       "├─Tanh: 1-17                             [1, 1, 256, 256]          --\n",
       "==========================================================================================\n",
       "Total params: 8,177,810\n",
       "Trainable params: 8,177,810\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 259.40\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 13.49\n",
       "Params size (MB): 32.71\n",
       "Estimated Total Size (MB): 46.46\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=unetgen, input_size=(1,1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
